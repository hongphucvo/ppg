{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replicate the dataset of this paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "1. Download all data using powershell (.ps1 file in each folder)\n",
    "2. Read the specify\n",
    "3. Mapping the fields\n",
    "4. Create script and execute and save the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion:\n",
    "dataset Dalia: \n",
    "- 15 participants \n",
    "- aged 30.6 +- 9.59 years (21–55)\n",
    "- gender: 7 male 8 female\n",
    "dataset PTT:\n",
    "- 22 participants\n",
    "- aged 28.52 +- (20-53)\n",
    "- gender: 16 male 6 female\n",
    "- healthy subjects performing 3 physical activities.\n",
    "\n",
    "<!-- Stat thêm về height weight -->\n",
    "\n",
    "This dataset is significantly healthy and young. Demographic has not as much balance and healthy -> not include illness condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_ptt_ppg_dataset(base_path):\n",
    "    \"\"\"\n",
    "    Load the Pulse Transit Time PPG Dataset v1.1.0\n",
    "    \n",
    "    Parameters:\n",
    "    base_path (str): Path to the PTT-PPG dataset folder\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing dataframes and metadata\n",
    "    \"\"\"\n",
    "    print(\"Loading PTT-PPG Dataset...\")\n",
    "    \n",
    "    # Initialize containers\n",
    "    data = {\n",
    "        'signals': [],\n",
    "        'demographics': [],\n",
    "        'subject_ids': [],\n",
    "        'activity_labels': []\n",
    "    }\n",
    "    \n",
    "    # Find all subject folders\n",
    "    subject_folders = glob.glob(os.path.join(base_path, 'S*'))\n",
    "    \n",
    "    for subject_folder in subject_folders:\n",
    "        subject_id = os.path.basename(subject_folder)\n",
    "        \n",
    "        # Load demographics if available\n",
    "        demo_file = os.path.join(subject_folder, f\"{subject_id}_info.csv\")\n",
    "        if os.path.exists(demo_file):\n",
    "            demo_df = pd.read_csv(demo_file)\n",
    "            demo_df['subject_id'] = subject_id\n",
    "            data['demographics'].append(demo_df)\n",
    "        \n",
    "        # Find all signal files\n",
    "        signal_files = glob.glob(os.path.join(subject_folder, \"*.csv\"))\n",
    "        for file in signal_files:\n",
    "            if \"_info.csv\" in file:\n",
    "                continue\n",
    "            \n",
    "            # Extract activity label from filename\n",
    "            filename = os.path.basename(file)\n",
    "            activity = filename.split('_')[1].split('.')[0] if '_' in filename else 'unknown'\n",
    "            \n",
    "            # Load signal data\n",
    "            try:\n",
    "                signal_df = pd.read_csv(file)\n",
    "                signal_df['subject_id'] = subject_id\n",
    "                signal_df['activity'] = activity\n",
    "                signal_df['dataset'] = 'PTT-PPG'\n",
    "                data['signals'].append(signal_df)\n",
    "                data['subject_ids'].append(subject_id)\n",
    "                data['activity_labels'].append(activity)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    # Combine all demographics\n",
    "    if data['demographics']:\n",
    "        data['demographics_df'] = pd.concat(data['demographics'], ignore_index=True)\n",
    "    else:\n",
    "        data['demographics_df'] = pd.DataFrame()\n",
    "    \n",
    "    # Combine all signals\n",
    "    if data['signals']:\n",
    "        data['signals_df'] = pd.concat(data['signals'], ignore_index=True)\n",
    "    else:\n",
    "        data['signals_df'] = pd.DataFrame()\n",
    "    \n",
    "    return data\n",
    "\n",
    "def load_ppg_dalia_dataset(base_path):\n",
    "    \"\"\"\n",
    "    Load the PPG-DaLiA dataset\n",
    "    \n",
    "    Parameters:\n",
    "    base_path (str): Path to the PPG-DaLiA dataset folder\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing dataframes and metadata\n",
    "    \"\"\"\n",
    "    print(\"Loading PPG-DaLiA Dataset...\")\n",
    "    \n",
    "    # Initialize containers\n",
    "    data = {\n",
    "        'signals': [],\n",
    "        'demographics': [],\n",
    "        'subject_ids': [],\n",
    "        'activity_labels': []\n",
    "    }\n",
    "    \n",
    "    # Find all subject files/folders\n",
    "    subject_folders = glob.glob(os.path.join(base_path, 'S*'))\n",
    "    \n",
    "    for subject_folder in subject_folders:\n",
    "        subject_id = os.path.basename(subject_folder)\n",
    "        \n",
    "        # Load demographics if available (assumed structure, adjust as needed)\n",
    "        demo_file = os.path.join(subject_folder, f\"{subject_id}_info.csv\")\n",
    "        if os.path.exists(demo_file):\n",
    "            demo_df = pd.read_csv(demo_file)\n",
    "            demo_df['subject_id'] = subject_id\n",
    "            data['demographics'].append(demo_df)\n",
    "        \n",
    "        # Find signal files - PPG-DaLiA typically has different structure\n",
    "        signal_files = glob.glob(os.path.join(subject_folder, \"**/*.csv\"), recursive=True)\n",
    "        \n",
    "        for file in signal_files:\n",
    "            filename = os.path.basename(file)\n",
    "            \n",
    "            # Skip demographic files\n",
    "            if \"info\" in filename.lower() or \"demographic\" in filename.lower():\n",
    "                continue\n",
    "                \n",
    "            # Extract activity label from path or filename\n",
    "            # This is an assumption - adjust based on actual file structure\n",
    "            if 'activity' in file.lower():\n",
    "                activity = file.split('activity_')[1].split('/')[0] if 'activity_' in file else 'unknown'\n",
    "            else:\n",
    "                activity = 'unknown'\n",
    "                \n",
    "            # Load signal data\n",
    "            try:\n",
    "                signal_df = pd.read_csv(file)\n",
    "                signal_df['subject_id'] = subject_id\n",
    "                signal_df['activity'] = activity\n",
    "                signal_df['dataset'] = 'PPG-DaLiA'\n",
    "                data['signals'].append(signal_df)\n",
    "                data['subject_ids'].append(subject_id)\n",
    "                data['activity_labels'].append(activity)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "    \n",
    "    # Combine all demographics\n",
    "    if data['demographics']:\n",
    "        data['demographics_df'] = pd.concat(data['demographics'], ignore_index=True)\n",
    "    else:\n",
    "        data['demographics_df'] = pd.DataFrame()\n",
    "    \n",
    "    # Combine all signals\n",
    "    if data['signals']:\n",
    "        data['signals_df'] = pd.concat(data['signals'], ignore_index=True)\n",
    "    else:\n",
    "        data['signals_df'] = pd.DataFrame()\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize data content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_column_names(df, mapping):\n",
    "    \"\"\"\n",
    "    Standardize column names based on mapping\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe\n",
    "    mapping (dict): Dictionary mapping original column names to standardized ones\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame: DataFrame with standardized column names\n",
    "    \"\"\"\n",
    "    new_cols = {}\n",
    "    for col in df.columns:\n",
    "        lower_col = col.lower()\n",
    "        for key, value in mapping.items():\n",
    "            if key in lower_col:\n",
    "                new_cols[col] = value\n",
    "                break\n",
    "    \n",
    "    if new_cols:\n",
    "        df = df.rename(columns=new_cols)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_datasets(ptt_ppg_data, ppg_dalia_data):\n",
    "    \"\"\"\n",
    "    Merge the two datasets into a standardized format\n",
    "    \n",
    "    Parameters:\n",
    "    ptt_ppg_data (dict): Dictionary containing PTT-PPG data\n",
    "    ppg_dalia_data (dict): Dictionary containing PPG-DaLiA data\n",
    "    \n",
    "    Returns:\n",
    "    dict: Dictionary containing merged data\n",
    "    \"\"\"\n",
    "    print(\"Merging datasets...\")\n",
    "    \n",
    "    # Define column name mappings for standardization\n",
    "    signal_mapping = {\n",
    "        'ppg': 'ppg_signal',\n",
    "        'red': 'ppg_red',\n",
    "        'ir': 'ppg_ir',\n",
    "        'green': 'ppg_green',\n",
    "        'ecg': 'ecg_signal',\n",
    "        'acc_x': 'accelerometer_x',\n",
    "        'acc_y': 'accelerometer_y',\n",
    "        'acc_z': 'accelerometer_z',\n",
    "        'gyro_x': 'gyroscope_x',\n",
    "        'gyro_y': 'gyroscope_y',\n",
    "        'gyro_z': 'gyroscope_z',\n",
    "        'time': 'timestamp'\n",
    "    }\n",
    "    \n",
    "    demographic_mapping = {\n",
    "        'age': 'age',\n",
    "        'gender': 'gender',\n",
    "        'height': 'height_cm',\n",
    "        'weight': 'weight_kg',\n",
    "        'bmi': 'bmi'\n",
    "    }\n",
    "    \n",
    "    # Standardize signal column names\n",
    "    if 'signals_df' in ptt_ppg_data and not ptt_ppg_data['signals_df'].empty:\n",
    "        ptt_ppg_data['signals_df'] = standardize_column_names(ptt_ppg_data['signals_df'], signal_mapping)\n",
    "    \n",
    "    if 'signals_df' in ppg_dalia_data and not ppg_dalia_data['signals_df'].empty:\n",
    "        ppg_dalia_data['signals_df'] = standardize_column_names(ppg_dalia_data['signals_df'], signal_mapping)\n",
    "    \n",
    "    # Standardize demographic column names\n",
    "    if 'demographics_df' in ptt_ppg_data and not ptt_ppg_data['demographics_df'].empty:\n",
    "        ptt_ppg_data['demographics_df'] = standardize_column_names(ptt_ppg_data['demographics_df'], demographic_mapping)\n",
    "    \n",
    "    if 'demographics_df' in ppg_dalia_data and not ppg_dalia_data['demographics_df'].empty:\n",
    "        ppg_dalia_data['demographics_df'] = standardize_column_names(ppg_dalia_data['demographics_df'], demographic_mapping)\n",
    "    \n",
    "    # Merge signals dataframes\n",
    "    signals_dfs = []\n",
    "    if 'signals_df' in ptt_ppg_data and not ptt_ppg_data['signals_df'].empty:\n",
    "        signals_dfs.append(ptt_ppg_data['signals_df'])\n",
    "    if 'signals_df' in ppg_dalia_data and not ppg_dalia_data['signals_df'].empty:\n",
    "        signals_dfs.append(ppg_dalia_data['signals_df'])\n",
    "    \n",
    "    merged_signals = pd.concat(signals_dfs, ignore_index=True) if signals_dfs else pd.DataFrame()\n",
    "    \n",
    "    # Merge demographics dataframes\n",
    "    demographics_dfs = []\n",
    "    if 'demographics_df' in ptt_ppg_data and not ptt_ppg_data['demographics_df'].empty:\n",
    "        demographics_dfs.append(ptt_ppg_data['demographics_df'])\n",
    "    if 'demographics_df' in ppg_dalia_data and not ppg_dalia_data['demographics_df'].empty:\n",
    "        demographics_dfs.append(ppg_dalia_data['demographics_df'])\n",
    "    \n",
    "    merged_demographics = pd.concat(demographics_dfs, ignore_index=True) if demographics_dfs else pd.DataFrame()\n",
    "    \n",
    "    # Create a comprehensive dataset with demographics joined to signals\n",
    "    if not merged_signals.empty and not merged_demographics.empty:\n",
    "        # Ensure both have subject_id columns for joining\n",
    "        if 'subject_id' in merged_signals.columns and 'subject_id' in merged_demographics.columns:\n",
    "            complete_dataset = pd.merge(\n",
    "                merged_signals, \n",
    "                merged_demographics, \n",
    "                on='subject_id', \n",
    "                how='left'\n",
    "            )\n",
    "        else:\n",
    "            complete_dataset = merged_signals\n",
    "            print(\"Warning: Could not merge demographics due to missing subject_id columns\")\n",
    "    else:\n",
    "        complete_dataset = merged_signals\n",
    "    \n",
    "    # Add a unique identifier column\n",
    "    complete_dataset['record_id'] = [f\"record_{i}\" for i in range(len(complete_dataset))]\n",
    "    \n",
    "    # Gather metadata about the merged dataset\n",
    "    metadata = {\n",
    "        'total_records': len(complete_dataset),\n",
    "        'unique_subjects': complete_dataset['subject_id'].nunique() if 'subject_id' in complete_dataset.columns else 0,\n",
    "        'datasets_included': ['PTT-PPG', 'PPG-DaLiA'],\n",
    "        'merge_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "        'columns': list(complete_dataset.columns)\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'merged_dataset': complete_dataset,\n",
    "        'merged_demographics': merged_demographics,\n",
    "        'merged_signals': merged_signals,\n",
    "        'metadata': metadata\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def save_merged_dataset(merged_data, output_path):\n",
    "    \"\"\"\n",
    "    Save the merged dataset and metadata to files\n",
    "    \n",
    "    Parameters:\n",
    "    merged_data (dict): Dictionary containing merged data\n",
    "    output_path (str): Path to save the output files\n",
    "    \"\"\"\n",
    "    print(\"Saving merged dataset...\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    \n",
    "    # Save complete dataset\n",
    "    if 'merged_dataset' in merged_data and not merged_data['merged_dataset'].empty:\n",
    "        merged_data['merged_dataset'].to_csv(os.path.join(output_path, 'merged_ppg_complete.csv'), index=False)\n",
    "    \n",
    "    # Save signals only\n",
    "    if 'merged_signals' in merged_data and not merged_data['merged_signals'].empty:\n",
    "        merged_data['merged_signals'].to_csv(os.path.join(output_path, 'merged_ppg_signals.csv'), index=False)\n",
    "    \n",
    "    # Save demographics only\n",
    "    if 'merged_demographics' in merged_data and not merged_data['merged_demographics'].empty:\n",
    "        merged_data['merged_demographics'].to_csv(os.path.join(output_path, 'merged_ppg_demographics.csv'), index=False)\n",
    "    \n",
    "    # Save metadata\n",
    "    if 'metadata' in merged_data:\n",
    "        metadata_df = pd.DataFrame([merged_data['metadata']])\n",
    "        metadata_df.to_csv(os.path.join(output_path, 'merged_ppg_metadata.csv'), index=False)\n",
    "    \n",
    "    print(f\"Merged dataset saved to {output_path}\")\n",
    "\n",
    "def main():\n",
    "    # Set paths to your datasets\n",
    "    ptt_ppg_path = '/path/to/ptt_ppg_dataset'  # Replace with your actual path\n",
    "    ppg_dalia_path = '/path/to/ppg_dalia_dataset'  # Replace with your actual path\n",
    "    output_path = '/path/to/output'  # Replace with your desired output path\n",
    "    \n",
    "    # Load datasets\n",
    "    ptt_ppg_data = load_ptt_ppg_dataset(ptt_ppg_path)\n",
    "    ppg_dalia_data = load_ppg_dalia_dataset(ppg_dalia_path)\n",
    "    \n",
    "    # Merge datasets\n",
    "    merged_data = merge_datasets(ptt_ppg_data, ppg_dalia_data)\n",
    "    \n",
    "    # Save merged dataset\n",
    "    save_merged_dataset(merged_data, output_path)\n",
    "    \n",
    "    print(\"Dataset merging complete!\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if 'metadata' in merged_data:\n",
    "        print(\"\\nMerged Dataset Summary:\")\n",
    "        print(f\"Total Records: {merged_data['metadata']['total_records']}\")\n",
    "        print(f\"Unique Subjects: {merged_data['metadata']['unique_subjects']}\")\n",
    "        print(f\"Datasets Included: {', '.join(merged_data['metadata']['datasets_included'])}\")\n",
    "        print(f\"Merge Date: {merged_data['metadata']['merge_date']}\")\n",
    "        print(f\"Number of Columns: {len(merged_data['metadata']['columns'])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading PTT-PPG Dataset...\n",
      "Loading PPG-DaLiA Dataset...\n",
      "Merging datasets...\n",
      "Saving merged dataset...\n",
      "Merged dataset saved to /path/to/output\n",
      "Dataset merging complete!\n",
      "\n",
      "Merged Dataset Summary:\n",
      "Total Records: 0\n",
      "Unique Subjects: 0\n",
      "Datasets Included: PTT-PPG, PPG-DaLiA\n",
      "Merge Date: 2025-03-30\n",
      "Number of Columns: 1\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
